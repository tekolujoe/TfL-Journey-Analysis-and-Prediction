# -*- coding: utf-8 -*-
"""London's_Travel _Network.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18B9vg3ixbwUfYmA051IVsqMuMkoivezH

**Project Description**

Transport for London (TfL) is a vast public transport network that allows London's residents to efficiently travel around the UK's capital, to the tune of over 1.5 million journeys per day!


**Complete Workflow:**

*    Load and Prepare Data
*    Feature Engineering
*    Model Evaluation
*    Hyperparameter Tuning
*    Final Model Evaluation and Visualization
"""

# Install packages
!pip install pandas
!pip install matplotlib
!pip install scikit-learn
!pip install statsmodels
!pip install seaborn
!pip install xgboost
!pip install catboost

# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from xgboost import XGBRegressor
from catboost import CatBoostRegressor

# Load the dataset
from google.colab import files
uploaded = files.upload()

# Read the dataset
df = pd.read_csv('tfl_journeys_final.csv')

# Display the first few rows of the dataframe
df.head()

df.info()

"""The dataset contains the following columns:

    month: Month of the year.
    year: Year of the record.
    days: Number of days in the month.
    report_date: Date when the report was generated.
    journey_type: Type of journey (e.g., Underground & DLR).
    journeys_millions: Number of journeys in millions.


From the output, we see that all columns except 'journeys_millions' have 936 non-null entries.
'journeys_millions' has 841 non-null entries, indicating that there are 95 missing values in this column.
"""

# Summary statistics
tfl_summary_stats = df.describe()
print(tfl_summary_stats)

# Check for missing values
missing_values = df.isnull().sum()
print(missing_values)

"""From the output above, we see that the missing values in the dataset are concentrated in the journeys_millions column, with 95 missing entries.

To handle the missing data, we will use interpolation.

**Why?**

Interpolation is used to handle missing values in the dataset by estimating values within the range of existing data points.

In the context of the journeys_millions column, interpolation helps in maintaining the continuity and trends in the data without introducing biases that might occur with other imputation methods like mean or median imputation.
"""

# Interpolate missing values in the 'journeys_millions' column
df['journeys_millions'] = df['journeys_millions'].interpolate()

# Verify if there are any remaining missing values
missing_values_after_interpolation = df.isnull().sum()
print(missing_values_after_interpolation)

df.info()

# Convert report_date to Datetime
df['report_date'] = pd.to_datetime(df['report_date'])

"""**Univariate Analysis**"""

plt.figure(figsize=(10, 6))
sns.histplot(df['journeys_millions'], bins=30, kde=True)
plt.title('Distribution of Journeys (Millions)')
plt.xlabel('Journeys (Millions)')
plt.ylabel('Frequency')
plt.show()

"""**Bivariate Analysis**"""

plt.figure(figsize=(14, 7))
sns.scatterplot(data=df, x='year', y='journeys_millions', hue='journey_type')
plt.title('Journeys Over the Years')
plt.xlabel('Year')
plt.ylabel('Journeys (Millions)')
plt.legend(title='Journey Type')
plt.grid(True)
plt.show()

"""**Time Series Analysis**"""

plt.figure(figsize=(14, 7))
sns.lineplot(data=df, x='year', y='journeys_millions', hue='journey_type', marker='o')
plt.title('Monthly Journeys Over the Years')
plt.xlabel('Year')
plt.ylabel('Journeys (Millions)')
plt.legend(title='Journey Type')
plt.grid(True)
plt.show()

"""**Correlation Analysis**"""

# Compute correlation matrix only for numerical columns
numerical_cols = df.select_dtypes(include=['float64', 'int64'])
correlation_matrix = numerical_cols.corr()

# Plot the correlation matrix
plt.figure(figsize=(10, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

# Monthly variation in journeys by journey type
df['month'] = df['report_date'].dt.month
plt.figure(figsize=(12, 6))
sns.boxplot(data=df, x='month', y='journeys_millions', hue='journey_type')
plt.title('Monthly Variation in Journeys by Type')
plt.xlabel('Month')
plt.ylabel('Journeys (Millions)')
plt.legend(title='Journey Type')
plt.show()

# Yearly Trends Analysis: Yearly journeys by journey type
yearly_data = df.groupby(['year', 'journey_type']).agg({'journeys_millions': 'sum'}).reset_index()
plt.figure(figsize=(14, 7))
sns.lineplot(data=yearly_data, x='year', y='journeys_millions', hue='journey_type', marker='o')
plt.title('Yearly Journeys by Type')
plt.xlabel('Year')
plt.ylabel('Journeys (Millions)')
plt.legend(title='Journey Type')
plt.grid(True)
plt.show()

# Outlier Detection: Boxplot of journeys by journey type
plt.figure(figsize=(12, 6))
sns.boxplot(data=df, x='journey_type', y='journeys_millions')
plt.title('Outlier Detection in Journeys by Type')
plt.xlabel('Journey Type')
plt.ylabel('Journeys (Millions)')
plt.grid(True)
plt.show()

# Aggregated Statistics - Mean, median, and standard deviation for each journey type
aggregated_stats = df.groupby('journey_type').agg({
    'journeys_millions': ['mean', 'median', 'std', 'min', 'max']
}).reset_index()
aggregated_stats.columns = ['Journey Type', 'Mean', 'Median', 'Standard Deviation', 'Min', 'Max']
print(aggregated_stats)

# Time Series Decomposition: Trend, seasonality, and residuals for each journey type
for journey_type in df['journey_type'].unique():
    journey_data = df[df['journey_type'] == journey_type]
    journey_data.set_index('report_date', inplace=True)
    journey_data = journey_data['journeys_millions'].resample('M').sum()

    decomposition = seasonal_decompose(journey_data, model='additive')
    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 8), sharex=True)

    ax1.plot(decomposition.observed, label='Observed')
    ax1.legend(loc='upper left')

    ax2.plot(decomposition.trend, label='Trend')
    ax2.legend(loc='upper left')

    ax3.plot(decomposition.seasonal, label='Seasonal')
    ax3.legend(loc='upper left')

    ax4.plot(decomposition.resid, label='Residual')
    ax4.legend(loc='upper left')

    fig.suptitle(f'Time Series Decomposition for {journey_type}', fontsize=16)
    plt.tight_layout()
    plt.subplots_adjust(top=0.92)
    plt.show()

# Visualize the number of journeys over the months and years
pivot_table = df.pivot_table(
    values='journeys_millions',
    index='year',
    columns='month',
    aggfunc='sum'
)
plt.figure(figsize=(12, 8))
sns.heatmap(pivot_table, cmap='YlGnBu', annot=True, fmt='.0f', linewidths=0.5)
plt.title('Heatmap of Monthly Data')
plt.xlabel('Month')
plt.ylabel('Year')
plt.show()

# Visualize pairwise relationships in the dataset
sns.pairplot(df, hue='journey_type')
plt.suptitle('Pairplot Analysis of TfL Journeys', y=1.02)
plt.show()

# Analyze trends and seasonality using rolling statistics
for journey_type in df['journey_type'].unique():
    journey_data = df[df['journey_type'] == journey_type]
    journey_data.set_index('report_date', inplace=True)
    journey_data = journey_data['journeys_millions'].resample('M').sum()

    rolling_mean = journey_data.rolling(window=12).mean()
    rolling_std = journey_data.rolling(window=12).std()

    plt.figure(figsize=(14, 7))
    plt.plot(journey_data, label='Original')
    plt.plot(rolling_mean, color='red', label='Rolling Mean')
    plt.plot(rolling_std, color='black', label='Rolling Std')
    plt.title(f'Rolling Statistics for {journey_type}')
    plt.legend(loc='best')
    plt.show()

"""**Machine Learning**"""

# Feature Engineering: Create lag features
df['month'] = df['report_date'].dt.month
df['year'] = df['report_date'].dt.year

# Create a pivot table to have a monthly time series
pivot_df = df.pivot_table(values='journeys_millions', index='report_date', columns='journey_type', aggfunc='sum')
pivot_df = pivot_df.asfreq('M').fillna(0)

print(pivot_df.head())

# Create lag features
for i in range(1, 13):
    pivot_df[f'lag_{i}'] = pivot_df['Underground & DLR'].shift(i)

# Drop rows with NaN values due to lagging
pivot_df = pivot_df.dropna()

# Define features (X) and target (y)
X = pivot_df.drop(columns=['Underground & DLR'])
y = pivot_df['Underground & DLR']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Define a function to evaluate models
def evaluate_model(model, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    print(f'Model: {model.__class__.__name__}')
    print(f'MAE: {mae}')
    print(f'MSE: {mse}')
    print(f'RMSE: {rmse}')
    print(f'R2 Score: {r2}')
    return model, y_pred, r2

# List of models to evaluate
models = [
    LinearRegression(),
    RandomForestRegressor(),
    GradientBoostingRegressor(),
    SVR(),
    DecisionTreeRegressor(),
    XGBRegressor(),
    CatBoostRegressor(verbose=0)
]

# Evaluate each model and select the best one based on R2 score
best_model = None
best_r2 = -np.inf
predictions = {}

for model in models:
    model_name = model.__class__.__name__
    print(f'\nEvaluating {model_name}...')
    evaluated_model, y_pred, r2 = evaluate_model(model, X_train, y_train, X_test, y_test)
    predictions[model_name] = y_pred
    if r2 > best_r2:
        best_r2 = r2
        best_model = evaluated_model

print(f'\nBest model is {best_model.__class__.__name__} with R2 score: {best_r2}')

# Hyperparameter tuning using GridSearchCV for the best model
if isinstance(best_model, GradientBoostingRegressor):
    param_grid = {
        'n_estimators': [100, 200],
        'learning_rate': [0.01, 0.1],
        'max_depth': [3, 5]
    }
elif isinstance(best_model, RandomForestRegressor):
    param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [3, 5, None]
    }
elif isinstance(best_model, XGBRegressor):
    param_grid = {
        'n_estimators': [100, 200],
        'learning_rate': [0.01, 0.1],
        'max_depth': [3, 5]
    }
elif isinstance(best_model, lgb.LGBMRegressor):
    param_grid = {
        'n_estimators': [100, 200],
        'learning_rate': [0.01, 0.1],
        'num_leaves': [31, 50]
    }
elif isinstance(best_model, CatBoostRegressor):
    param_grid = {
        'iterations': [100, 200],
        'learning_rate': [0.01, 0.1],
        'depth': [3, 5]
    }
else:
    param_grid = {}

if param_grid:
    grid_search = GridSearchCV(best_model, param_grid, cv=5, scoring='r2', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_
    print(f'\nBest parameters from GridSearchCV: {grid_search.best_params_}')

# Evaluate the best model from GridSearchCV
_, y_pred_best, _ = evaluate_model(best_model, X_train, y_train, X_test, y_test)

# Plot actual vs predicted values for the best model
plt.figure(figsize=(14, 7))
plt.plot(y_test.index, y_test, label='Actual')
plt.plot(y_test.index, y_pred_best, label='Predicted', linestyle='--')
plt.xlabel('Date')
plt.ylabel('Journeys (Millions)')
plt.title('Actual vs Predicted Journeys')
plt.legend()
plt.show()

# Plot feature importance for CatBoost
if isinstance(best_model, CatBoostRegressor):
    feature_importances = best_model.get_feature_importance()
    feature_names = X.columns
    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})
    importance_df = importance_df.sort_values(by='Importance', ascending=False)

    plt.figure(figsize=(12, 8))
    sns.barplot(x='Importance', y='Feature', data=importance_df)
    plt.title('Feature Importance for CatBoost Regressor')
    plt.show()